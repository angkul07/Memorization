# Reducing Memorization Without Compromising Generalization in LLMs via Prompt-Driven Perturbation Training

> A lightweight, inference-driven approach to reduce memorization in large language models (LLMs) using GPT-4o-powered semantic perturbations and automatic filtering.

---

## ðŸ” Motivation

Modern large language models (LLMs) like GPT-4o demonstrate powerful reasoning abilities, especially in few-shot or in-context learning (ICL) settings. However, they remain vulnerable to **verbatim memorization** of training dataâ€”posing **privacy risks** and casting doubt on whether their outputs reflect true reasoning.

**This project proposes an inference-time data curation strategy that reduces memorization without compromising generalization**, leveraging the LLM ecosystem itself (GPT-4o + DeepEval/g-eval).

---

## Research Objective

We introduce **Perturbation-Aware Prompting (PAP)**â€”a novel training-time intervention that curates a **memorization-resilient training set** using LLM-generated semantic perturbations and filtering via g-eval. Our key hypotheses:

- **H1**: Models trained on perturbed data will exhibit **lower memorization**, measured using Adversarial Confidence Rating (ACR).
- **H2**: These models will **retain generalization** on unseen test samples, maintaining high reasoning accuracy.

---

## Key Contributions

- âš™ï¸ **Perturbation Framework**: Use GPT-4o to generate lexical, typographical, and structural perturbations of GSM8K answers.
- ðŸ§  **Automatic Filtering with g-eval**: Retain only semantically valid perturbations using DeepEval's rule-based verification.
- ðŸ§ª **Memorization Assessment**: Apply ACR to detect memorization on training samples.
- ðŸ“ˆ **Generalization Benchmark**: Evaluate models on the GSM8K test set to ensure no accuracy drop.

---

## Methodology Overview

### 1. Dataset Preparation
- **Base**: [GSM8K dataset](https://huggingface.co/datasets/gsm8k)
- **Perturbations**:
  - Synonym swaps (lexical)
  - Minor typos (typographical)
  - Sentence reorderings (structural)

### 2. Perturbation Generation
- Prompts to GPT-4o create multiple perturbed answers per GSM8K sample.
- Stored and versioned under [`perturbation_generation_code/`](./datasets/perturbation_generation_code).

### 3. Filtering with g-eval
- Use DeepEvalâ€™s `g-eval` to verify:
  - Semantic consistency
  - Rule satisfaction
- Only pass-verified samples form the **GSM8K-P dataset** (`g-eval_filtered_perturbed_data/`).

### 4. Model Training & Evaluation
- **Variants**:
  - `Vanilla`: Trained on original GSM8K
  - `Perturbed`: Trained on GSM8K-P
- Evaluate using:
  - `ACR` (memorization)
  - GSM8K test accuracy (generalization)
 
> Every trained model is available [here](https://huggingface.co/sohamwasmatkar)

---

## Repository Structure

```
ðŸ“¦ project-root
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ g-eval_filtered_perturbed_data/      # g-eval filtered data which contains perturbed + original dataset mix
â”‚   â””â”€â”€ generalization_accuracy_data/        # Datasets contains accuracy labels dataset.
â”‚
â”œâ”€â”€ perturbation_generation_code/            # Jupyter notebooks for generating + filtering perturbations
â”‚   â”œâ”€â”€ GSM8K_Perturbed.ipynb
â”‚   â”œâ”€â”€ v1 dataset creation pipeline.ipynb
â”‚   â””â”€â”€ example perturbations.ipynb
â”‚
â”œâ”€â”€ eval_metric/                              # Code for mia-tuner and different evaluation metrics
â”‚   â””â”€â”€ acr_code/                             # Code for Adversarial Confidence Rating (ACR)
|       â”œâ”€â”€ acr-memorization/
â”‚       â”œâ”€â”€ acr.py
â”‚   â”œâ”€â”€ generalization_accuracy.ipynb
â”‚   â”œâ”€â”€ mia_hybrid.py
â”‚   â””â”€â”€ memgen.py
â”‚
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Evaluation Metrics

| Metric              | Description                                                                | Goal        |
|---------------------|-----------------------------------------------------------------------------|-------------|
| **ACR (â†“)**         | Model confidence on training set prompts (lower = less memorization)       | â†“ Lower     |
| **Test Accuracy (â†‘)** | Reasoning performance on GSM8Kâ€™s 1k test set (higher = better generalization) | â†‘ Higher    |
| **g-eval Pass Rate (â†‘)** | % of perturbations passing semantic rule checks via g-eval             | â†‘ Higher    |

---
