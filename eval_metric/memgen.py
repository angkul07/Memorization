import numpy as np
import random
import json
import torch
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftConfig, PeftModel


def calculate_perplexity_on_model_outputs():
    """
    Calculate perplexity on the outputs generated by the model, not on the input questions.
    This function:
    1. Loads each model and its corresponding dataset
    2. Generates outputs for questions in both training and test sets
    3. Calculates perplexity on those generated outputs
    4. Computes the perplexity ratio (PR) between test and training outputs
    
    Returns:
        list: List of perplexity ratio values for each model
    """
    # Model and dataset mappings
    model_dataset_pairs = [
        ("sohamwasmatkar/lora_model_0", "gsm8k_0.csv"),
        ("sohamwasmatkar/lora_model_25", "filtered_gsmk8_25.csv"),
        ("sohamwasmatkar/lora_model_50", "filtered_gsmk8_50.csv"),
        ("sohamwasmatkar/lora_model_75", "filtered_gsmk8_75.csv"),
        ("sohamwasmatkar/lora_model_100", "filtered_gsmk8_100.csv")
    ]

    # Create a dictionary to store scores by model
    model_scores = {}
    for model_name, _ in model_dataset_pairs:
        model_scores[model_name] = {"train": None, "test": None}

    # Step 1: Generate answers from the model and calculate perplexity on those answers
    for model_name, dataset_path in model_dataset_pairs:
        print(f"Processing model {model_name} with dataset {dataset_path}")

        try:
            # Load the PEFT config
            peft_config = PeftConfig.from_pretrained(model_name)
            
            # Configure quantization
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16
            )
            
            # Get base model name
            base_model_name = peft_config.base_model_name_or_path
            
            # Load base model with quantization
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                quantization_config=bnb_config,
                device_map="auto"
            )
            
            # Load tokenizer
            tokenizer = AutoTokenizer.from_pretrained(base_model_name)
            tokenizer.pad_token = tokenizer.eos_token
            
            # Create PeftModel
            model = PeftModel.from_pretrained(base_model, model_name)
            model.eval()  # Set to evaluation mode
            
            # Load questions from dataset
            questions = []
            try:
                if dataset_path.endswith('.csv'):
                    import pandas as pd
                    df = pd.read_csv(dataset_path)
                    if 'question' in df.columns:
                        questions = df['question'].tolist()
                    elif 'original_question' in df.columns:
                        questions = df['original_question'].tolist()
                # Add handling for other file types if needed
            except Exception as e:
                print(f"Error loading dataset {dataset_path}: {e}")
                continue
                
            if not questions:
                print(f"No questions found in {dataset_path}, skipping...")
                continue
                
            # Sample questions for efficiency
            sample_size = min(100, len(questions))
            sampled_questions = random.sample(questions, sample_size)
            
            # Generate answers and calculate perplexity on them
            train_perplexities = []
            
            for question in sampled_questions:
                try:
                    # Generate model output/answer
                    inputs = tokenizer(question, return_tensors="pt").to(model.device)
                    
                    # Generate the answer
                    with torch.no_grad():
                        output_ids = model.generate(
                            inputs["input_ids"],
                            max_new_tokens=256,
                            do_sample=False,
                            pad_token_id=tokenizer.eos_token_id
                        )
                    
                    # Extract only the generated part (not including the input prompt)
                    input_length = inputs["input_ids"].shape[1]
                    generated_ids = output_ids[0, input_length:]
                    
                    # If no tokens were generated, skip
                    if len(generated_ids) == 0:
                        continue
                        
                    # Get the generated text
                    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
                    
                    # Now calculate perplexity on the generated text
                    # Re-encode the generated text
                    eval_inputs = tokenizer(generated_text, return_tensors="pt").to(model.device)
                    
                    # Set up labels for loss calculation
                    labels = eval_inputs["input_ids"].clone()
                    
                    # Calculate perplexity
                    with torch.no_grad():
                        outputs = model(input_ids=eval_inputs["input_ids"], labels=labels)
                        
                        # Get loss and calculate perplexity
                        if outputs.loss is not None:
                            ppl = torch.exp(outputs.loss).item()
                            train_perplexities.append(ppl)
                            print(f"Generated text: '{generated_text[:100]}...' - Perplexity: {ppl}")
                        
                except Exception as e:
                    print(f"Error processing question: {e}")
                    continue
            
            if train_perplexities:
                avg_ppl = np.mean(train_perplexities)
                model_scores[model_name]["train"] = avg_ppl
                print(f"Average perplexity on generated outputs for {model_name}: {avg_ppl}")
            
            # Clean up
            del model
            del base_model
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"Error processing model {model_name}: {e}")
            continue
    
    # Step 2: Calculate perplexity on test set outputs
    try:
        gsm8k_test = load_dataset("gsm8k", "main", split="test")
        gsm8k_test = gsm8k_test.select(range(min(50, len(gsm8k_test))))
    except Exception as e:
        print(f"Error loading GSM8K test dataset: {e}")
        return []
        
    for model_name, _ in model_dataset_pairs:
        if model_scores[model_name]["train"] is None:
            continue
            
        print(f"Processing test data for {model_name}")
        
        try:
            # Load model components as before
            peft_config = PeftConfig.from_pretrained(model_name)
            
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16
            )
            
            base_model_name = peft_config.base_model_name_or_path
            
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                quantization_config=bnb_config,
                device_map="auto"
            )
            
            tokenizer = AutoTokenizer.from_pretrained(base_model_name)
            tokenizer.pad_token = tokenizer.eos_token
            
            model = PeftModel.from_pretrained(base_model, model_name)
            model.eval()
            
            # Calculate perplexity on test set
            test_perplexities = []
            
            for test_example in gsm8k_test:
                try:
                    question = test_example['question']
                    
                    # Generate model output/answer for test question
                    inputs = tokenizer(question, return_tensors="pt").to(model.device)
                    
                    with torch.no_grad():
                        output_ids = model.generate(
                            inputs["input_ids"],
                            max_new_tokens=256,
                            do_sample=False,
                            pad_token_id=tokenizer.eos_token_id
                        )
                    
                    # Extract only the generated part
                    input_length = inputs["input_ids"].shape[1]
                    generated_ids = output_ids[0, input_length:]
                    
                    if len(generated_ids) == 0:
                        continue
                        
                    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)
                    
                    # Calculate perplexity on generated text
                    eval_inputs = tokenizer(generated_text, return_tensors="pt").to(model.device)
                    
                    labels = eval_inputs["input_ids"].clone()
                    
                    with torch.no_grad():
                        outputs = model(input_ids=eval_inputs["input_ids"], labels=labels)
                        
                        if outputs.loss is not None:
                            ppl = torch.exp(outputs.loss).item()
                            test_perplexities.append(ppl)
                            
                except Exception as e:
                    print(f"Error processing test example: {e}")
                    continue
                    
            if test_perplexities:
                avg_ppl = np.mean(test_perplexities)
                model_scores[model_name]["test"] = avg_ppl
                print(f"Average perplexity on test outputs for {model_name}: {avg_ppl}")
                
            # Clean up
            del model
            del base_model
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"Error processing test data for model {model_name}: {e}")
            continue
            
    # Step 3: Calculate Perplexity Ratios (PR)
    pr_values = []
    model_pr_dict = {}
    
    for model_name, scores in model_scores.items():
        if scores["train"] is not None and scores["test"] is not None:
            pr = scores["test"] / scores["train"]
            pr_values.append(pr)
            model_pr_dict[model_name] = pr
            print(f"Perplexity Ratio (PR) for {model_name}: {pr}")
            
    # Print summary
    print("\nPerplexity Ratio Summary:")
    for model_name, pr in model_pr_dict.items():
        print(f"{model_name}: {pr}")
        
    return pr_values


def calculate_mgti(asr_train, pr, acc_test):
    """
    Calculate the MGTI using the formula:
    MGTI = ASR_train + PR - Acc_test

    Args:
        asr_train: The Adversarial Success Rate on the training set
        pr: The Perplexity Ratio between test and training data
        acc_test: The accuracy on the test set

    Returns:
        The calculated MGTI value
    """
    mgti = asr_train + pr - acc_test
    return mgti


if __name__ == "__main__":
    # Calculate PR values for each model
    pr_values = calculate_perplexity_on_model_outputs()
    
    if pr_values:
        # Print individual PR values
        print("\nIndividual Perplexity Ratios:")
        for i, pr in enumerate(pr_values):
            print(f"PR for model {i}: {pr}")
            
        # Calculate average PR
        pr_avg = sum(pr_values) / len(pr_values)
        print(f"\nAverage Perplexity Ratio: {pr_avg}")
        
        # Example usage of MGTI calculation
        # Replace with actual values when available
        asr_train = 0.0  # Placeholder
        acc_test = 0.0   # Placeholder
        mgti = calculate_mgti(asr_train, pr_avg, acc_test)
        print(f"\nMGTI Calculation:")
        print(f"MGTI = ASR_train + PR - Acc_test")
        print(f"MGTI = {asr_train} + {pr_avg} - {acc_test} = {mgti}")
    else:
        print("Unable to calculate perplexity ratios due to missing values.")
